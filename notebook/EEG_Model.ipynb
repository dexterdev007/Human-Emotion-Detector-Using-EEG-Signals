{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b95748",
   "metadata": {},
   "source": [
    "## MLDC Mapping\n",
    "\n",
    "1. **Problem Definition** → Emotion detection from EEG\n",
    "2. **Data Collection** → Load EEG CSVs from `dataset/features_raw.csv`\n",
    "3. **Data Processing** → Missing values + scaling\n",
    "4. **EDA** → Shape, samples, statistics\n",
    "5. **Feature Engineering** → Raw EEG channels (baseline)\n",
    "6. **Model Selection** → Linear + Logistic Regression\n",
    "7. **Deployment** → Exported to web UI in this project\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed04cb8",
   "metadata": {},
   "source": [
    "### Dataset Used (dataset/features_raw.csv)\n",
    "- 32 EEG channels per row (time samples).\n",
    "- No real labels included → we generate **synthetic** valence/arousal/dominance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8bbb15",
   "metadata": {},
   "source": [
    "## 1) Problem Definition\n",
    "We want to predict emotion dimensions from EEG: **valence**, **arousal**, **dominance**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7e6576",
   "metadata": {},
   "source": [
    "## 2) Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0287b84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=RuntimeWarning)\n",
    "np.seterr(all='ignore')\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c18484",
   "metadata": {},
   "source": [
    "## 3) Data Loading\n",
    "Load the EEG data file with channel columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd0b034",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"../dataset/features_raw.csv\")\n",
    "# Drop empty column if it exists\n",
    "if 'Unnamed: 32' in data.columns:\n",
    "    data = data.drop(columns=['Unnamed: 32'])\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244afadd",
   "metadata": {},
   "source": [
    "## 4) EDA (Basic Exploration)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "545d1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df8bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe().loc[[\"mean\", \"std\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6438fb",
   "metadata": {},
   "source": [
    "## 5) Preprocessing\n",
    "- Fill missing values\n",
    "- Scale features for ML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e8885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean extreme values to avoid numeric overflow\n",
    "data = data.replace([np.inf, -np.inf], np.nan)\n",
    "# First fill with column means, then zero-fill any remaining NaNs\n",
    "try:\n",
    "    data = data.fillna(data.mean(numeric_only=True))\n",
    "except TypeError:\n",
    "    data = data.fillna(data.mean())\n",
    "data = data.fillna(0)\n",
    "\n",
    "# Clip outliers per column (1st–99th percentile)\n",
    "try:\n",
    "    lower = data.quantile(0.01, numeric_only=True)\n",
    "    upper = data.quantile(0.99, numeric_only=True)\n",
    "except TypeError:\n",
    "    lower = data.quantile(0.01)\n",
    "    upper = data.quantile(0.99)\n",
    "\n",
    "data = data.clip(lower=lower, upper=upper, axis=1)\n",
    "\n",
    "X = data.to_numpy(dtype=float)\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Ensure no NaN/inf remains, then clip to stable range\n",
    "X = np.nan_to_num(X, nan=0.0, posinf=5.0, neginf=-5.0)\n",
    "X = np.clip(X, -5, 5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964f8f6",
   "metadata": {},
   "source": [
    "## 6) Feature Selection / Creation\n",
    "We use **raw scaled EEG channels** as baseline features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cffef1d",
   "metadata": {},
   "source": [
    "## 7) Create Synthetic Valence/Arousal/Dominance Labels\n",
    "Because real emotion labels are missing, we create **pseudo-labels** using EEG patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d3fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: scale any signal to 1–9 range (safe)\n",
    "def scale_1_9(x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    x = np.nan_to_num(x, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    min_v = x.min()\n",
    "    max_v = x.max()\n",
    "    denom = max_v - min_v\n",
    "    if denom == 0:\n",
    "        return np.full_like(x, 5.0, dtype=float)\n",
    "    scaled = (x - min_v) / denom * 8 + 1\n",
    "    return np.clip(scaled, 1, 9)\n",
    "\n",
    "# Valence: frontal asymmetry (right - left)\n",
    "valence_raw = (data['F4'] + data['Fp2']) - (data['F3'] + data['Fp1'])\n",
    "\n",
    "# Arousal: overall absolute activity\n",
    "arousal_raw = data.abs().mean(axis=1)\n",
    "\n",
    "# Dominance: central + parietal activity (simple heuristic)\n",
    "dom_channels = ['C3','C4','P3','P4','Pz']\n",
    "dominance_raw = data[dom_channels].abs().mean(axis=1)\n",
    "\n",
    "# Scale to 1–9\n",
    "y_val = scale_1_9(valence_raw)\n",
    "y_ar = scale_1_9(arousal_raw)\n",
    "y_dom = scale_1_9(dominance_raw)\n",
    "\n",
    "# Final safety cleanup\n",
    "y_val = np.nan_to_num(y_val, nan=5.0, posinf=9.0, neginf=1.0)\n",
    "y_ar = np.nan_to_num(y_ar, nan=5.0, posinf=9.0, neginf=1.0)\n",
    "y_dom = np.nan_to_num(y_dom, nan=5.0, posinf=9.0, neginf=1.0)\n",
    "\n",
    "# Binary labels (High vs Low)\n",
    "y_val_bin = (y_val > np.median(y_val)).astype(int)\n",
    "y_ar_bin = (y_ar > np.median(y_ar)).astype(int)\n",
    "y_dom_bin = (y_dom > np.median(y_dom)).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fec19e",
   "metadata": {},
   "source": [
    "## 8) Train/Test Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fff0f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous splits\n",
    "X_train, X_test, yv_train, yv_test = train_test_split(X, y_val, test_size=0.2, random_state=42)\n",
    "_, _, ya_train, ya_test = train_test_split(X, y_ar, test_size=0.2, random_state=42)\n",
    "_, _, yd_train, yd_test = train_test_split(X, y_dom, test_size=0.2, random_state=42)\n",
    "\n",
    "# Binary splits\n",
    "X_train_b, X_test_b, yv_train_b, yv_test_b = train_test_split(X, y_val_bin, test_size=0.2, random_state=42)\n",
    "_, _, ya_train_b, ya_test_b = train_test_split(X, y_ar_bin, test_size=0.2, random_state=42)\n",
    "_, _, yd_train_b, yd_test_b = train_test_split(X, y_dom_bin, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fae072",
   "metadata": {},
   "source": [
    "## 9) Linear Regression (Intensity Scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9956d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = LinearRegression()\n",
    "\n",
    "# Valence\n",
    "lin.fit(X_train, yv_train)\n",
    "pred_v = lin.predict(X_test)\n",
    "print('Valence MSE:', mean_squared_error(yv_test, pred_v))\n",
    "\n",
    "# Arousal\n",
    "lin.fit(X_train, ya_train)\n",
    "pred_a = lin.predict(X_test)\n",
    "print('Arousal MSE:', mean_squared_error(ya_test, pred_a))\n",
    "\n",
    "# Dominance\n",
    "lin.fit(X_train, yd_train)\n",
    "pred_d = lin.predict(X_test)\n",
    "print('Dominance MSE:', mean_squared_error(yd_test, pred_d))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507bf71a",
   "metadata": {},
   "source": [
    "## 10) Logistic Regression (High vs Low)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de9816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Valence High/Low\n",
    "log.fit(X_train_b, yv_train_b)\n",
    "pred_vb = log.predict(X_test_b)\n",
    "print('Valence Accuracy:', accuracy_score(yv_test_b, pred_vb))\n",
    "\n",
    "# Arousal High/Low\n",
    "log.fit(X_train_b, ya_train_b)\n",
    "pred_ab = log.predict(X_test_b)\n",
    "print('Arousal Accuracy:', accuracy_score(ya_test_b, pred_ab))\n",
    "\n",
    "# Dominance High/Low\n",
    "log.fit(X_train_b, yd_train_b)\n",
    "pred_db = log.predict(X_test_b)\n",
    "print('Dominance Accuracy:', accuracy_score(yd_test_b, pred_db))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (VSCode)",
   "language": "python",
   "name": "vscode39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
